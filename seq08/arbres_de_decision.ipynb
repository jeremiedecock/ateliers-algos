{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6e89cf-721c-4ab0-86ab-78447212a8c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Decision trees and random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654dbd3-914b-4ba2-b4f8-effd5c01481b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b220f-7c6f-4449-bf45-d427fae2a3c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a6fca-94e5-4fcc-a789-4542f2f54612",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "from IPython.display import Image   # To display graphviz images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3cd65-4826-4335-9b71-88cd75c77173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78dda44-b8b1-43ca-8fa3-50aa5b8b95a0",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What are decision trees?\n",
    "\n",
    "- Non-parametric supervised learning methods\n",
    "- For both classification and regression tasks\n",
    "  - *Attributes* (a.k.a features or variables): can be categorical, numerical or binary\n",
    "  - *Labels* (a.k.a outputs): can be categorical, numerical or binary\n",
    "- Learn simple if-then-else decision rules that can be represented on a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a08a91-487f-4cf1-8375-3450eb982648",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What are decision trees?\n",
    "\n",
    "Tree Structure:\n",
    "- Represents a hierarchy of decisions\n",
    "- Each internal node denotes a test on an *attribute*\n",
    "- Each branch represents an outcome of the test\n",
    "- Leaf nodes hold the final decision or prediction\n",
    "\n",
    "A tree can be seen as a piecewise constant approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bfacd-10ea-4b68-9f63-d79f721c0b5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dot -Tsvg tree2.dot > figs/tree2.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e98ec-066d-45a9-ace7-7ce6fb77b55c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example (here for binary classification with categorical attributes)\n",
    "\n",
    "<img src=\"figs/tree2.svg\" width=\"60%\" />\n",
    "\n",
    "**Voc**:\n",
    "\n",
    "- *Attributes* (features or variables)\n",
    "- Outcome\n",
    "- Examples\n",
    "- Labels (outputs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a16008-a417-4d28-aff7-3e2ad6a27dfa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Image('tree1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697f86d-e24f-422d-9e5a-4abf1cf9cd82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"dataset_golf_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6467cc-9d37-45b6-8d47-9b6b29c566e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## How to automatically build a decision tree from a dataset?\n",
    "\n",
    "**What do we want**\n",
    "1. A tree that accurately predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3418391-9b62-4b74-8880-e8cae590e6e1",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "There are many trees that predict examples from a dataset with equivalent accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509007-6a4a-48ae-a0cc-10508953d591",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How to automatically build a decision tree from a dataset?\n",
    "\n",
    "**What do we want**\n",
    "1. A tree that accurately predicts\n",
    "2. *A tree as simple as possible*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bc41c-1c04-4c78-ad3d-34b1220badb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Naive algorithms\n",
    "\n",
    "- Brute force\n",
    "\n",
    "We could proceed by brutforce, testing all possible trees for a given set of attributes and measuring their size and accuracy, but this is not feasible in practice: the number of possible trees grows exponentially with the number of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8b7cf-c47a-44be-8eb1-f786146e33f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Naive algorithms\n",
    "\n",
    "- Brute force\n",
    "- Evolutionary algorithms\n",
    "\n",
    "We could use evolutionary algos, but here too we'll quickly be limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ffcb4-fb4f-4a52-9091-97e07b8882ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Greedy algorithms\n",
    "\n",
    "In practice, greedy method is recursively used to build a decision tree from a dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9358c7-baba-4cf4-9876-04b1e270d9dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "1. **Selection of the Best Attribute**: At each step in the algorithm, ID3 chooses the attribute that is most useful for classifying the data. This is done using a measure like Information Gain or Gain Ratio. The attribute with the highest Information Gain (or another chosen metric) is selected as the decision node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bbafc-42b4-455c-97ab-db44857e9aeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "2. **Tree Construction**:\n",
    "  - Start with all the training instances and a set of all the attributes.\n",
    "  - Choose the best attribute using a greedy strategy (highest Information Gain, for example).\n",
    "  - Make that attribute a decision node and divide the dataset into smaller subsets based upon the values of this attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0289-a115-446f-8360-12f429c481c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "3. **Recursive Splitting**:\n",
    "  - For each subset of data (which is now smaller than the original set):\n",
    "    - If all instances in the subset belong to the same class or there are no more attributes to be selected, then create a leaf node with the class label.\n",
    "    - If there are mixed instances, then repeat the process: choose the best attribute for this subset of data and split it further. This is the recursive part, where the algorithm repeats the process of attribute selection and tree construction for each new subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefccd1-724e-44df-9aca-86f6460e0c63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "4. **Termination**: The recursion is terminated when either all instances at a node belong to the same class, there are no more attributes left to split upon, or the tree reaches a predefined depth limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cbf96-5567-493a-8acc-9073ba6d8eea",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Recursive construction of a (binary) decision tree\n",
    "\n",
    "**PROCEDURE** Build-tree (node X)\n",
    "<br>\n",
    "**IF** All points in X belong to the same class then <br>\n",
    "$\\quad$ Create a leaf bearing the name of this class <br>\n",
    "**ELSE** <br>\n",
    "$\\quad$ Choose the best attribute to create a node <br>\n",
    "$\\quad$ The test associated with this node splits $X$ into two parts, denoted $X_l$ and $X_r$ <br>\n",
    "$\\quad$ *Build-tree*($X_l$) <br>\n",
    "$\\quad$ *Build-tree*($X_r$) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d457fa6-0365-47be-ac30-a2cc2b14b5f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How do you \"choose the best attribute to create a node\"?\n",
    "\n",
    "We use 2 concepts for this:\n",
    "- *Entropy*\n",
    "- *Information gain*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b3a55-8931-4cb4-8992-5b306d23bcd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Entropy\n",
    "\n",
    "The notion of entropy is borrowed from information theory (Shannon).\n",
    "\n",
    "Shannon, Claude Elwood. \"A mathematical theory of communication.\" The Bell system technical journal 27, no. 3 (1948): 379-423. [PDF](https://cse.buffalo.edu/~hungngo/classes/2003/Markov_Chains/papers/p3-shannon.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82535590-bdde-44b7-8432-ce1246dfe41f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Entropy is a measure of uncertainty or variability in data.\n",
    "The higher the entropy, the more disordered and varied the information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ef938-f3d7-4e23-be0c-47e1d6b11efe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In decision trees, entropy helps determine how to divide (or \"split\") the data so as to make the subsets as \"pure\" as possible.\n",
    "In the context of classification, a \"pure\" subset would contain data of a single class or type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfafa3-02df-4a4d-ba29-c6b2fa902f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{Entropy}(S) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "Entropy is measured in bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e9424-ea43-4c2c-9b8d-0ba12c5c1c53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###Â Exercise\n",
    "\n",
    "Compute the entropy of the following subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab305d-40bb-46d0-a2b5-9bd336e2ffc5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_golf_1.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985d508-b9a2-4c8f-b0d9-ba459306beec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7d6e8-2fe6-492a-a07d-55ec84922e73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{Entropy}(S_1) = - p_{\\text{don't play}} \\log_2 p_{\\text{don't play}} - p_{\\text{play}} \\log_2 p_{\\text{play}} = - \\frac22 \\log_2 \\frac22 - \\frac02 \\log_2 \\frac02 = 0 ~ \\text{bit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47ea26-b8c6-49f2-ba5f-fc6890072b54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45c152-218a-401c-917d-7028fca18929",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{Entropy}(S_2) = - p_{\\text{don't play}} \\log_2 p_{\\text{don't play}} - p_{\\text{play}} \\log_2 p_{\\text{play}} = - \\frac24 \\log_2 \\frac24 - \\frac24 \\log_2 \\frac24 = 1 ~ \\text{bit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dac4df-051c-47ae-a94f-7cbcfd46c940",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d8763-bf3f-404a-84fa-9ecab0a1afae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{Entropy}(S_2) = - p_{\\text{don't play}} \\log_2 p_{\\text{don't play}} - p_{\\text{play}} \\log_2 p_{\\text{play}} = - \\frac25 \\log_2 \\frac25 - \\frac35 \\log_2 \\frac35 = 0.97 ~ \\text{bit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ad2eb-1558-41e0-980c-9e5408fc7226",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "See also: https://scikit-learn.org/stable/modules/tree.html#classification-criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fd2f1-3298-4c2e-8a58-394ca9d7e2c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Information gain\n",
    "\n",
    "Entropy tells us \"how pure a subset is\" but it doesn't actually tells us how to *choose* the attribute.\n",
    "\n",
    "We use a second metric for this: **information gain**.\n",
    "\n",
    "The information gain Gain(S,A) for a set S and an attribute A is calculated as the difference between the entropy of the entire set S and the weighted sum of the entropy of each subset of S created by splitting on attribute A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3244168-83f0-4c25-9c3c-0cadc8245c45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{InformationGain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- Entropy(S) is the entropy of the set S.\n",
    "- Values(A) are the different values that attribute A can take.\n",
    "- Svâ€‹ is the subset of S for which attribute A has value v.\n",
    "- âˆ£Svâˆ£ is the size of subset Svâ€‹, and âˆ£Sâˆ£ is the size of the set S.\n",
    "\n",
    "This formula calculates how much \"information\" is gained by splitting the data on attribute A.\n",
    "The goal in ID3 is to choose the attribute that maximizes this information gain at each step of building the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c17a91-8dab-447d-b74a-83071c897a05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How / when does the algorithm stop splitting ?\n",
    "\n",
    "There are three options for selecting when a decision tree algorithm stops splitting:\n",
    "\n",
    "1. Allow the tree to split until every subset is pure. This means that, if necessary, the algorithm will keep splitting until each end node (leaf) subset contains 1 example and is therefore 100% pure. This might seem desirable, but it can lead to a problem known as overfitting, which we will cover in the next chapter.\n",
    "2. Stop the tree from splitting until every leaf subset is pure. This might seem like a good option, but it can quickly lead to a high error rate and poor performance because the tree is not robust enough.\n",
    "3. Adopt a stopping method. This is the when to stop splitting used by decision tree algorithms to determine when to stop splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb41a7-b740-406b-bf14-2b23fb754db8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Depending on the type of tree you are using, there are multiple approaches to choose from. Some of these include:\n",
    "\n",
    "- Stopping when a tree reaches a maximum number of levels, or depth.\n",
    "- Stopping when a minimum information-gain level is reached.\n",
    "- Stopping when a subset contains less than a defined number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57394540-ad32-41c2-9719-f96edc56c019",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329ffb9-40f4-49d2-8342-703892f55972",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implement the entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14080085-5f36-40f7-83b0-b1d86f9508a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(target_col: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_col : np.ndarray\n",
    "        The target column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The entropy of the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts)) * np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e605f-81e9-4e8a-ae62-db6c1a625a31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Test the entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f202f6-de02-45ca-b692-35dd27f8ac2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_golf_1.csv\", dtype=str)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e575a53-e5cc-4352-ab5f-6e85ede776d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### On all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91116a5f-128f-4ac7-b86d-00c8475f9954",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = df.label.values\n",
    "print(f\"Entropy of subset {labels.tolist()} = {entropy(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35ef6c-32ec-4df9-9dcc-ede6e54fd9cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "####Â On a subset of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd558636-b900-4ba3-a5d3-abb3085330cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = df.label.values[:2]\n",
    "print(f\"Entropy of subset {labels.tolist()} = {entropy(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded033e-858d-480e-963a-b214e2c9b5fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "####Â On another subset of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f4440-d9ea-4250-b8af-307f4755a111",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = df.label.values[:4]\n",
    "print(f\"Entropy of subset {labels.tolist()} = {entropy(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456877-ee04-475b-ace0-0aca93cebd3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implement the info_gain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908dc26-7ba2-4642-b3da-cdfa9a3dc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(data: pd.DataFrame, split_attribute_name: str, target_name: Optional[str] = \"class\") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset for whose feature the IG should be calculated\n",
    "    split_attribute_name : str\n",
    "        The name of the feature for which the information gain should be calculated\n",
    "    target_name : str, optional\n",
    "        The name of the target feature. The default is \"class\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The information gain of the dataset\n",
    "    \"\"\"\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate the values and the corresponding counts for the split attribute \n",
    "    vals, counts= np.unique(data[split_attribute_name], return_counts=True)\n",
    "    \n",
    "    # Calculate the weighted entropy\n",
    "    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) * entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    # Calculate the information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e721a-94fa-46aa-a20a-bbe6812a98dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Test the info_gain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab999c-f364-4a5a-b491-9dfe4478af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(df, \"outlook\", target_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd106fc-bce1-4b52-bfe0-37b5b506173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(df, \"humidity\", target_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a78c2d-1039-413c-9300-10cb5d818109",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(df, \"wind\", target_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6e985-86d2-422d-8b2a-d5de4c55be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(df, \"temperature\", target_name=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1ee53-fef2-43cf-ae8f-82df324bbdf7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implement the id3_algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601d521-ec32-4605-bef9-23d6805e1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_algorithm(data: pd.DataFrame, original_data: pd.DataFrame, features: List[str], target_attribute_name: str = \"class\", parent_node_class: Optional[Any] = None) -> Any:\n",
    "    \"\"\"\n",
    "    ID3 Algorithm. This function takes five parameters:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The data for which the ID3 algorithm should be applied\n",
    "    original_data : pd.DataFrame\n",
    "        This is the original dataset needed to calculate the mode target feature value of the original dataset in the case the dataset delivered by the first parameter is empty\n",
    "    features : List[str]\n",
    "        The feature space of the dataset. This is needed for the recursive call since during the tree growing process\n",
    "    target_attribute_name : str, optional\n",
    "        The name of the target attribute. The default is \"class\"\n",
    "    parent_node_class : Any, optional\n",
    "        This is the value or class of the mode target feature value of the parent node for a specific node. This is also needed for the recursive call in the case the dataset is empty. The default is None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The prediction result\n",
    "    \"\"\"\n",
    "    # Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
    "    \n",
    "    # If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(original_data[target_attribute_name])[np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # If the feature space is empty, return the mode target feature value of the direct parent node --> Note that the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
    "    # the mode target feature value is stored in the parent_node_class variable.\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    # If none of the above holds true, grow the tree!\n",
    "    else:\n",
    "        # Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select the feature which best splits the dataset\n",
    "        item_values = [info_gain(data, feature, target_attribute_name) for feature in features] # Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        # Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            # Split the dataset along the value of the feature with the largest information gain and therewith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = id3_algorithm(sub_data, dataset, features, target_attribute_name, parent_node_class)\n",
    "            \n",
    "            # Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be9173-464b-42a3-93c5-3977efa94eeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Test the id3_algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c95b12-9024-4e0e-bd19-0870c226716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dataset' is a pandas DataFrame containing your dataset\n",
    "dataset = pd.read_csv(\"dataset_golf_1.csv\")\n",
    "\n",
    "# The features (attributes) are the column names of the dataset (except the target feature)\n",
    "features = dataset.columns[:-1]\n",
    "\n",
    "# The target\n",
    "target_attribute_name = \"label\"\n",
    "\n",
    "# Train the tree\n",
    "decision_tree = id3_algorithm(dataset, dataset, features, target_attribute_name=target_attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7150f-1c2c-476f-8b54-1efff28a008e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afed795-df8e-4d0b-97d2-0d5339393b1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implement a function to plot the tree (WIP ðŸ˜…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04353c59-0a31-40ec-9d71-cf5629ad39a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def add_nodes_edges(tree, parent_name, graph):\n",
    "    if isinstance(tree, dict):\n",
    "        for node, subtree in tree.items():\n",
    "            graph.node(node)\n",
    "            if parent_name is not None:\n",
    "                graph.edge(parent_name, node)\n",
    "            add_nodes_edges(subtree, node, graph)\n",
    "    else:\n",
    "        # Leaf node\n",
    "        graph.node(tree)\n",
    "        if parent_name is not None:\n",
    "            graph.edge(parent_name, tree)\n",
    "\n",
    "def tree_to_dot(tree):\n",
    "    graph = Digraph()\n",
    "    add_nodes_edges(tree, None, graph)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ff95b-93b8-4392-912c-0167365fdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = tree_to_dot(decision_tree)\n",
    "dot.render('decision_tree.dot', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb12e2a-040c-4273-9dbb-f59a55b33d0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<img src=\"decision_tree.dot.svg\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7bd8b-cf45-43a2-997a-e1a8cea09771",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implement the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273de670-2333-40c8-a56b-0d331c231bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query: Dict[str, Any], tree: Dict[str, Any], default: Optional[int] = 1) -> Any:\n",
    "    \"\"\"\n",
    "    Prediction of a new/unseen query instance. This takes three parameters:\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : Dict[str, Any]\n",
    "        A dictionary of the shape {\"feature_name\":feature_value,...}\n",
    "    tree : Dict[str, Any]\n",
    "        The tree that was trained on the training data (ID3)\n",
    "    default : int, optional\n",
    "        The prediction that will be returned if the query instance is not applicable in the tree. Default is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The prediction result\n",
    "    \"\"\"\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "  \n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e632a-f30f-4d48-9d54-487ef31e8895",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Test the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3a36e-b70a-4890-be61-96c2196785ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a new instance by calling the 'predict' function\n",
    "query = dataset.iloc[0,:].to_dict()\n",
    "query.pop(target_attribute_name)\n",
    "\n",
    "prediction = predict(query, tree)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df20f11-baa6-4ee3-915f-0dea2e5a359f",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## One example of alternative selection criteria\n",
    "### Gini Impurity\n",
    "\n",
    "$$\n",
    "Gini(T) = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "\n",
    "- T represent a training dataset\n",
    "- p is the probability of \"T\" belonging to class \"i\"\n",
    "\n",
    "The lower the impurity the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72575c0e-f244-48c8-bfa3-2359a7c695a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Decision Tree Algorithms Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472b3cf-c38c-4bcb-a877-b5db40f31c74",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- **ID3 (Iterative Dichotomiser 3)**\n",
    "  - **entropy** and **information gain**\n",
    "  - Developed in 1979 by Ross Quinlan.\n",
    "  - Creates a multiway tree.\n",
    "  - Selects categorical features at each node for the largest information gain.\n",
    "  - Trees grown to maximum size, then pruned for better generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6337e-9682-48e4-957a-e7aff3467dde",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- **C4.5 (Successor to ID3)**\n",
    "  - **entropy** and **gain ratio**\n",
    "  - Developed in 1986 by Ross Quinlan.\n",
    "  - Handles both categorical and continuous features.\n",
    "  - Converts decision trees into if-then rules.\n",
    "  - Rules are ordered based on accuracy.\n",
    "  - Prunes rules by evaluating accuracy improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c25cd-3edc-4a2e-a012-303dd0c96059",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- **C5.0 (Latest Version by Quinlan)**\n",
    "  - Proprietary license.\n",
    "  - More efficient in memory and rule size than C4.5.\n",
    "  - Higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5c444-37f3-41db-9839-16ea9339eb5c",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- **CART (Classification and Regression Trees)**\n",
    "  - **gini impurty** or **variance reduction**\n",
    "  - Developed in 1984 by Brieman, Friedman, Ohlson and Stone\n",
    "  - Similar to C4.5, but supports numerical targets for regression.\n",
    "  - Builds binary trees based on largest information gain.\n",
    "  - Does not generate rule sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf0504-0be4-4502-b16e-4df72cc91cf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Potential problems with decision trees\n",
    "\n",
    "- Overfitting\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640273c8-f85c-47e5-a3cf-1494803b13c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"dataset_golf_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d203f99-ae6d-42e7-b28a-21fd7acb6d84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"dataset_golf_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e35ae-1974-4a29-9f9b-73d2a934bae7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset_golf_1.csv\")\n",
    "features = dataset.columns[:-1]\n",
    "target_attribute_name = \"label\"\n",
    "decision_tree = id3_algorithm(dataset, dataset, features, target_attribute_name=target_attribute_name)\n",
    "\n",
    "decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3a68f-d2a9-4370-915b-1c786ec7a9f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dot = tree_to_dot(decision_tree)\n",
    "dot.render('decision_tree1.dot', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a82f3-796d-48dd-8cd1-ebfcd1b93de9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<img src=\"decision_tree1.dot.svg\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad235e3f-ec16-475d-8b98-337a6e1340be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset_golf_2.csv\")\n",
    "features = dataset.columns[:-1]\n",
    "target_attribute_name = \"label\"\n",
    "decision_tree = id3_algorithm(dataset, dataset, features, target_attribute_name=target_attribute_name)\n",
    "\n",
    "decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a08439-3888-4028-827e-49b80f968596",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dot = tree_to_dot(decision_tree)\n",
    "dot.render('decision_tree2.dot', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04290f0-d361-4652-bd3d-7c2943171533",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<img src=\"decision_tree2.dot.svg\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b6914-b6da-44d5-863c-93f4fe09f93b",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generalization\n",
    "\n",
    "- Statistical sifificance tests\n",
    "- Pruning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03030581-2aa6-44e2-a6ae-ebeee13df8a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Regression [TODO]\n",
    "\n",
    "<img src=\"figs/arbres_decision_regression_representation_donnees_numeriques.png\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa794a9-0919-41ad-bd0c-d244d16c744d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Scikit-Learn implementation\n",
    "\n",
    "### Models\n",
    "\n",
    "- **Classification** `sklearn.tree.DecisionTreeClassifier` -> [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "- **Regression** `sklearn.tree.DecisionTreeRegressor` -> [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "Scikit-Learn uses an optimized version of the CART algorithm\n",
    "\n",
    "See also: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93861330-e237-407b-8cc6-ba143d2d3317",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Warning\n",
    "\n",
    "The Scikit-Learn implementation does not support categorical variables for now!\n",
    "\n",
    "See: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde0b6e-7258-420c-bb17-559875dce8a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Classification: Example with the Iris dataset\n",
    "\n",
    "- **Number of Instances**: 150 (50 in each of three classes)\n",
    "- **Number of Attributes**: 4 numeric, predictive attributes and the class\n",
    "- **Attribute Information**:\n",
    "  - *sepal length* in cm (min: 4.3, max: 7.9)\n",
    "  - *sepal width* in cm  (min: 2.0, max: 4.4)\n",
    "  - *petal length* in cm (min: 1.0, max: 6.9)\n",
    "  - *petal width* in cm  (min: 0.1, max: 2.5)\n",
    "- **class**:\n",
    "  - Iris-Setosa (33.3% of the dataset)\n",
    "  - Iris-Versicolour (33.3% of the dataset)\n",
    "  - Iris-Virginica (33.3% of the dataset)\n",
    "\n",
    "(see [doc1](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) and [doc2](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb18a0-96de-4be4-9ed5-c038b353bdce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92121ded-4c86-4d49-a56c-1ed1a5bfc465",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f270ea-c920-4b33-8c6a-66b0c909629d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d3b87-c805-4f6d-9961-6820be20f157",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60df2ec-18f4-43ec-82e2-c0a1f02e33be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree.plot_tree(clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269cfca-02fe-4132-b39e-340c36edb7fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.predict([[5.84, 3.05, 3.76, 1.20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c037f6-28a7-4fcc-a653-e7499fe5b66e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import graphviz    # !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fdc9a-eac3-4bc9-9446-80584d7ebedf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=dataset.feature_names,  \n",
    "                                class_names=dataset.target_names,  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7bd13-c786-46ca-85d7-61a3babeb78e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = dataset.data[:, pair]\n",
    "    y = dataset.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    ax = plt.subplot(2, 3, pairidx + 1)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax,\n",
    "        xlabel=dataset.feature_names[pair[0]],\n",
    "        ylabel=dataset.feature_names[pair[1]],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(\n",
    "            X[idx, 0],\n",
    "            X[idx, 1],\n",
    "            c=color,\n",
    "            label=dataset.target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\n",
    "plt.legend(loc=\"lower right\", borderpad=0, handletextpad=0);\n",
    "#_ = plt.axis(\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d3430-2e21-471d-a224-d915796cfaac",
   "metadata": {},
   "source": [
    "### Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe25fd-59f4-4b87-a18b-d9c8fa2159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e70a4-a7fd-4207-b9b2-ebfd3942d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = tree.DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = tree.DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96da19-156a-4b03-a043-6e6fee17b3cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##Â Advantages of Decision Trees\n",
    "\n",
    "- **Easy to Understand and Interpret**: Trees can be visualized, making them easy to understand and interpret, even for non-technical stakeholders.\n",
    "- **Handles Both Numerical and Categorical Data**: They can handle datasets that have both numerical and categorical variables.\n",
    "- **No Need for Data Preprocessing**: Often requires little data preparation. They do not require normalization of data or dummy variables.\n",
    "- **Non-Parametric Method**: Since they are non-parametric, they are not constrained by a particular distribution of data.\n",
    "- **Automatic Feature Selection**: Decision trees implicitly perform feature selection during training, which is beneficial in cases with a large number of features- Can Model Non-Linear Relationships: Effective at capturing non-linear relationships between features and labels.\n",
    "- **Useful for Exploratory Analysis**: Can be used to identify the most influential variables in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478cfe8-8dee-4b42-b9a3-b50cc667d75f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##Â Disadvantages of Decision Trees\n",
    "\n",
    "- **Overfitting**: Tend to overfit the data, especially if the tree is deep with many branches. This can be mitigated using techniques like pruning, setting a maximum depth, or minimum samples per leaf.\n",
    "- **Variance**: Small variations in the data can result in a completely different tree. This can be reduced by using ensemble methods like Random Forests.\n",
    "- **Not Ideal for Continuous Variables**: They are not the best choice for continuous numerical data, as they lose information when categorizing variables into different nodes.\n",
    "- **Biased with Imbalanced Data**: Decision trees can be biased towards dominant classes, so they might require balancing before being used.\n",
    "- **Instability**: A small change in the data can lead to a significant change in the structure of the decision tree, making them quite unstable.\n",
    "- **Greedy Algorithms**: Decision trees use a greedy approach which might not result in the globally optimal tree.\n",
    "- **Difficulty in Capturing Complex Relationships**: They may struggle to capture more complex relationships without becoming overly complex themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4aaba-58c7-4891-b02d-1668022352e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "- R. Quinlan, \"Learning efficient classification procedures\", Machine Learning: an artificial intelligence approach, Michalski, Carbonell & Mitchell (eds.), Morgan Kaufmann, 1983, p. 463-482.\n",
    "- R. Quinlan, \"The effect of noise on concept learning\", Machine Learning: an artificial intelligence approach, Vol. II, Michalski, Carbonell & Mitchell (eds.), Morgan Kaufmann, 1986, p. 149-166.\n",
    "- R. Quinlan, \"Induction of decision trees\", Machine learning, 1 (1), p. 81-106, Kluwer.\n",
    "- J. Cheng, U. Fayyad, K. Irani, Z. Quian, \"Improved decision trees: a generalized version of ID3\", International ML Conference, 1988, Ann-Arbor, p. 100-106.\n",
    "- P. Utgoff, \"ID5: an incremental ID3\", International ML Conference, 1988, Ann-Arbor, p. 107-120.\n",
    "- J. Wirth, J. Catlett, \"Experiments on the Costs and Benefits of Windowing in ID3\", International ML Conference, 1988, Ann-Arbor, p. 87-99.\n",
    "- Breiman, Leo. \"Bagging predictors.\" Machine learning 24 (1996): 123-140. [PDF](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf)\n",
    "- Breiman, Leo. \"Random forests.\" Machine learning 45 (2001): 5-32. [PDF](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf)\n",
    "- Breiman, Leo. Classification and regression trees. Routledge, 2017.\n",
    "- Chen, Tianqi, and Carlos Guestrin. \"Xgboost: A scalable tree boosting system.\" In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794. 2016. [PDF](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785)\n",
    "- Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of statistics (2001): 1189-1232. [PDF](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.pdf)\n",
    "- Mason, Llew, Jonathan Baxter, Peter Bartlett, and Marcus Frean. \"Boosting algorithms as gradient descent.\" Advances in neural information processing systems 12 (1999). [PDF](https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
