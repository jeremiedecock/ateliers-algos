{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF7Q8jEeEPM1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=darkblue> Optimization using second order gradient descent </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG6cr03iSYt6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this notebook, we provide an **overview of second order gradient-based optimization algorithms** which completes the first session **on first order methods and accelerated algorithms**. Supervised learning applications are usually based on the minimization of an objective function on $\\mathbb{R}^d$ (kernel based SVM models, penalized regression, maximum likelihood estimation of neural networks) and accelerated gradient methods are the go-to solutions to solve these optimization problems.\n",
    "\n",
    "The results provided in this notebook are valid with assumptions on the target functions such as convexity or strong convexity (some of them may be relaxed). Although these algorithms are widely used, keep in mind that these assumptions do not hold in practice and that  non-convexity stems from the arbitrary form of the loss functions used in machine learning.\n",
    "\n",
    "This will motivate **additional sessions on alternative methods such as evolutionary approaches for difficult non-linear non-convex optimisation problems** in continuous and discrete domains (for instance the CMA-ES algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv1tTxcDjLW6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <font color=darkred> Bibliography & additional ressources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLsK5IHEjPWb",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [1] Convex Optimization, S. Boyd & L. Vandenberghe, 2009, https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n",
    "``Very complete book on convex optimization and gradient descent algorithms (full gradient, Newton's method, constrained problems)``\n",
    ", etc.\n",
    "- [2] Convex Optimization: Algorithms and\n",
    "Complexity, S. Bubeck, 2015, https://arxiv.org/pdf/1405.4980.pdf\n",
    "\n",
    "-  [3] Probabilistic machine learning: an introduction, Kevin P. Murphy, 2022, https://probml.github.io/pml-book/book1.html\n",
    "``Full book online with all basics on machine learning. Not state-of-the-art but very good introduction``\n",
    "\n",
    "- [4] Learning theory from first principles, F. Bach, 2023, https://www.di.ens.fr/~fbach/ltfp_book.pdf\n",
    "``Much more advanced reference, Chapter 5 on optimization``\n",
    "\n",
    "- [5] Algorithms for optimization, M. J. Kochenderfer and T. A. Wheeler, 2019.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B61KWCTMHjsy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred>Introduction : general framework & motivations</font>\n",
    "\n",
    "Parameter inference often boils down to solving\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{argmin}_{\\theta \\in \\mathbb{R}^d} \\,\\left\\{\\ell_n(\\theta)=\\frac 1n \\sum_{i=1}^n \\ell(\\theta, y_i, x_i) + \\lambda \\mathrm{pen}(\\theta)\\right\\}\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\lambda>0$, $\\mathrm{pen}(\\cdot)$ some penalization function and $(x_i,y_i)_{1\\leq i\\leq n}$ are ``training examples of inputs and outputs`` (in a supervised setting), and $\\theta$ is an ``unknown parameter to be estimated.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Examples of penalization functions**:\n",
    "\n",
    "- $\\mathrm{pen}(\\theta) =  \\|\\theta\\|_2^2$ (Ridge for regularization).\n",
    "\n",
    "- $\\mathrm{pen}(\\theta) = \\|\\theta\\|_1$ (Lasso for sparsity),\n",
    "see Tibshirani, R. (1996). \n",
    "``Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288, and all extensions``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8502,
     "status": "ok",
     "timestamp": 1685965004542,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "SQHBC-8OJxKK",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29640\\1467931600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import autograd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 25,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \"#93a1a1\",\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ0d_yRIRIDw",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Simple cost function to test the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EOokWxB4IRB",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We may consider the Booth and Rozenbrock functions, see fo instance [5], to test the optimization procedures presented in this session:\n",
    "$$\n",
    "f_{\\mathrm{booth}} : (x_1,x_2) \\mapsto (x_1+2x_2-7)^2 + (2x_1+x_2-5)^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "f_{\\mathrm{roz}} : (x_1,x_2) \\mapsto (1-x_1)^2 + 100(x_2-x_1^2)^2\\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1685965133282,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "QzXanW6X7Pc3",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def rozenbrock(x):\n",
    "    return (1-x[0])**2 + 100*(x[1] - x[0]**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1685965133283,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "inBtkl2WGp56",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def booth(x, y):\n",
    "    return (x + 2*y - 7)**2 + (2*x + y - 5)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1685965159333,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "aaUCRfa2WqTi",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "executionInfo": {
     "elapsed": 1831,
     "status": "ok",
     "timestamp": 1685965163242,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "7w6hF96QGqBY",
    "outputId": "cc3cfd2e-24ad-41f9-a21d-3f6eb0007e4e",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "x = np.arange(-4, 4, 0.1)\n",
    "y = np.arange(-4, 4, 0.1)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = booth(x,y)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxDZ_IL_GF4v",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Newton's method </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0oEotU_NHkP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $\\theta_k$ be the current parameter estimate. We consider the second order Taylor expansion of the target $\\ell_n$. \n",
    "\n",
    "More precisely, this approach proposes to locally **approximate the function $\\ell_n$ around $\\theta_k$ by a second-order Taylor expansion**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From https://francisbach.com/self-concordant-analysis-newton/\n",
    "![](https://drive.google.com/uc?export=view&id=1aQz3wROslBIqRB9ih_zr3LEES6LWPS0J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the case where $\\nabla^2 \\ell_n(\\theta_k)$ is definite positive, the function\n",
    "$$\n",
    "\\theta\\mapsto \\ell_n(\\theta_k) + \\theta^\\top\\nabla\\ell_n(\\theta_k) + \\frac{1}{2}\\theta^\\top\\nabla^2 \\ell_n(\\theta_k)\\theta\n",
    "$$\n",
    "admits a unique minimum at \n",
    "$$\n",
    "\\theta_* = -(\\nabla^2 \\ell_n(\\theta_k))^{-1}\\nabla\\ell_n(\\theta_k)\\,.\n",
    "$$\n",
    "This motivates the following iterative algorithm:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - (\\nabla^2 \\ell_n(\\theta_k))^{-1}\\nabla\\ell_n(\\theta_k)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juGqbmMcP52f",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Implementation from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1685965510806,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "0nvHgIkiDTuc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685965511526,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "H7RttK4rF1jK",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def NR_update(f, x0):\n",
    "    g = autograd.jacobian(f)\n",
    "    h = autograd.hessian(f)\n",
    "\n",
    "    x = x0\n",
    "    \n",
    "    gx = np.squeeze(g(x))\n",
    "    hx = np.squeeze(h(x))\n",
    "    \n",
    "    #####\n",
    "    # A compléter\n",
    "    delta = np.linalg.multi_dot([np.linalg.inv(hx), gx])\n",
    "    x = x - delta\n",
    "    #####\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685965511528,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "JDHIpBjkGZ35",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def NR_loop(f, x0, maxiter=50):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    f: function to be optimized\n",
    "    x0: initial parameter estimate\n",
    "    maxiter: maximum number of updates\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    params: estimators\n",
    "    loglik: sequence of loglikelihood along iterations\n",
    "    \"\"\"\n",
    "    params = x0\n",
    "    loglik = f(x0)\n",
    "    for _ in range(maxiter):\n",
    "        new_params = NR_update(f, params)\n",
    "        loglik = np.append(loglik,f(new_params))\n",
    "        params = new_params\n",
    "        \n",
    "    return params,  loglik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "radpjQWr6G6G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Compute the gradient and the Hessian of the Booth function. \n",
    "\n",
    "- Write an iteration of the Newton update for this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz_BD9jy91dF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "####\n",
    "# A compléter\n",
    "# Ecrire l'optimisation par la méthode de Newton de la fonction de Rozenbrock\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 2007,
     "status": "ok",
     "timestamp": 1685965517979,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "uq-9wRlfQJ4r",
    "outputId": "042ce3c9-413e-4258-cc60-8c5b4737a764",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rozenbrock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29640\\1153862472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrozenbrock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexp_it\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mloglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNR_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rozenbrock' is not defined"
     ]
    }
   ],
   "source": [
    "f = rozenbrock\n",
    "iterations = 10\n",
    "for exp_it in range(25):\n",
    "    x0 = 5*np.random.randn(2)\n",
    "    params,  loglik = NR_loop(f, x0, iterations)\n",
    "    plt.plot(loglik, '-', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhz2ybCpQcCm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### **Convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uUq9t2INi1D",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assum that the target function $f$ is convex and twice differentiable. Assume also that:\n",
    "- $\\nabla f$ is $L$-Lipschitz ;\n",
    "- $\\nabla^2 f$ is $M$-Lipschitz ;\n",
    "- $f$ is strongly convex with parameter $m$.\n",
    "\n",
    "Then, there exists a constant $c$, for all $k>0$,\n",
    "$$\n",
    "f(x_{k+k_0}) - f(x_*) \\leq c \\left(\\frac{1}{2}\\right)^{2^k}\\,,\n",
    "$$\n",
    "where $k_0$ is the number of steps until $\\|\\nabla f(x_{k_0+1})\\|$ is below a fixed threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Fa_9FBKZls",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Locally, the method is quadratically convergent, that is, there exists $c>0$, such that if $\\|x_k –x_*\\|\\leq c$, then $\\|x_{k+1} –x_*\\|/c\\leq (\\|x_k –x_*\\|/c)^2$. Thies yields\n",
    "$$\n",
    "\\|x_k –x_*\\|⩽c(\\|x_{k_0} –x_*\\|/c)^{2^{k-k_0}},\n",
    "$$\n",
    "where $k_0$ is such that $\\|x_k –x_*\\|\\leq c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47rtfTg21XPJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Newton's method is **sensitive to initial conditions**, in particular for non-convex objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Giv6s2a134l",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Newton’s method is **very computationally intensive**. The computation of the inverse Hessian scales as **$O(d^3)$** which is prohibitive in high dimensional settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH-_7I7GT7Wd",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###### **Built-in optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8204,
     "status": "ok",
     "timestamp": 1685965738037,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "QEWZjl-rVp-7",
    "outputId": "0c883012-c6cf-4976-f4d3-892e2908aced",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting autograd-minimize\n",
      "  Downloading autograd_minimize-0.2.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from autograd-minimize) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->autograd-minimize) (1.22.4)\n",
      "Installing collected packages: autograd-minimize\n",
      "Successfully installed autograd-minimize-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd-minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1685965738449,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "KOxAH73RT6i_",
    "outputId": "0a692ccf-6907-4c5a-bbea-2ea3d1486820",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.0\n",
       "        x: [ 1.000e+00  1.000e+00]\n",
       "      nit: 81\n",
       "      jac: [ 0.000e+00  0.000e+00]\n",
       " hess_inv: [[ 4.975e-01  9.950e-01]\n",
       "            [ 9.950e-01  1.995e+00]]\n",
       "     nfev: 102\n",
       "     njev: 102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from autograd_minimize import minimize\n",
    "x0 = 5*np.random.randn(2)\n",
    "minimize(f, x0, backend='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4818,
     "status": "ok",
     "timestamp": 1685965977238,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "hHtZ0_4yWbkz",
    "outputId": "db589caa-443c-4ef3-f6b1-d951268e6322",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully.\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: 3.485150501411498e-27\n",
       "       x: [ 1.000e+00  1.000e+00]\n",
       "     nit: 37\n",
       "     jac: [ 2.344e-12 -1.177e-12]\n",
       "    nfev: 46\n",
       "    njev: 46\n",
       "    nhev: 56"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(f, x0, method='Newton-CG', precision='float64', tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kr6kYipGenw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Quasi Newton - Broyden-Fletcher-Goldfarb-Shanno (BFGS) method</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FpBRuVQx5f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computation of the Hessian matrix is sometimes **intractable or computationally very sensitive**. Quasi-Newton approaches propose to use an update similar to the Newton Raphson algorithm with an approximation of the Hessian, i.e.\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\gamma_k A_k^{-1}\\nabla\\ell_n(\\theta_k)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMSGdNb2oiM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this framework, $A_k$ aims at approximating $\\nabla^2 \\ell_n(\\theta_k)$. This approximation satisfies in general the quasi-Newton condition:\n",
    "$$\n",
    "A_{k+1}(\\theta_{k+1} - \\theta_k) = \\nabla \\ell_n(\\theta_{k+1}) - \\nabla \\ell_n(\\theta_{k})\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND-cEwafRPCK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the BFGS setting, the approximation of the Hessian matrix is computed recursively as follows.\n",
    "- $\\delta_k = \\theta_{k+1}-\\theta_k$.\n",
    "- $d_k = \\nabla\\ell_n(\\theta_{k+1}) -\\nabla\\ell_n(\\theta_k)$. \n",
    "- $A_{k+1} = A_k + \\left(d_k^\\top \\delta_k\\right)^{-1}d_kd_k^\\top - \\left(\\delta_k^\\top A_k\\delta_k\\right)^{-1}A_k\\delta_k(A_k\\delta_k)^\\top$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk3htXX-TwL1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using **Woodbury formula**, we can directly compute $A_{k+1}^{-1}$ as follows.\n",
    "\\begin{align*}\n",
    "\\alpha_k &= (d_k^\\top \\delta_k)^{-1}\\,,\\\\\n",
    "A_{k+1}^{-1} &= \\left(I - \\alpha_k \\delta_k d_k^\\top\\right)A_{k}^{-1}\\left(I - \\alpha_k d_k\\delta_k^\\top\\right) + \\alpha_k \\delta_k \\delta_k^\\top\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feH5WLEcYRvC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### **Line search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk62oPCoYSCt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing the step length $\\gamma_k$ amounts to solving a tradeoff between decreasing the value of $\\ell_n$ and not spending too much time at each iteration.\n",
    "\n",
    "A popular line search condition targets to choose $\\gamma_k$ such that:\n",
    "$$\n",
    "\\ell_n(\\theta_k + \\gamma_kp_k ) \\leq \\ell_n (\\theta_k ) + c_1 \\gamma_k \\nabla \\ell_n(\\theta_k)^\\top p_k\\,,\n",
    "$$\n",
    "for $p_k = -A_k^{-1}\\nabla\\ell_n(\\theta_k)$ some constant $0<c_1<1$.\n",
    "\n",
    "A second requirement to avoid too short steps requires that\n",
    "$$\n",
    "\\nabla \\ell_n(\\theta_k + \\gamma_k p_k )^\\top\n",
    "p_k \\geq c_2 \\nabla \\ell_n(\\theta_k)^\\top p_k\n",
    "$$\n",
    "where $c_1<c_2<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S62gptpbRdMQ",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###### **Implementation from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfva2s5Tg1xS",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def line_search(f,x,p,nabla):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    f: function to be optimized\n",
    "    x: current parameter estimate\n",
    "    p: current value of p\n",
    "    nabla: current value of the gradient \n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    gamma: step-size \n",
    "    \"\"\"\n",
    "    g = autograd.jacobian(f)\n",
    "    \n",
    "    gamma = 1\n",
    "    c1 = 1e-4 \n",
    "    c2 = 0.9 \n",
    "    \n",
    "    fx = f(x)\n",
    "    x_new = x + gamma * p \n",
    "    nabla_new = g(x_new)\n",
    "    ###\n",
    "    # A compléter\n",
    "    ####\n",
    "    while f(x_new) >= fx + (c1*gamma*nabla.T@p) or nabla_new.T@p <= c2*nabla.T@p : \n",
    "        gamma *= 0.5\n",
    "        x_new = x + gamma * p \n",
    "        nabla_new = g(x_new)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1685783515602,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "qb1xANmrGifT",
    "outputId": "01c36aa1-377f-4112-b912-c0d2d84b1be0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached!\n"
     ]
    }
   ],
   "source": [
    "def BFGS_loop(f,x0,max_it):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ----------\n",
    "    f: function to be optimised \n",
    "    x0: starting point\n",
    "    max_it: maximum number of iterations\n",
    "\n",
    "    Outputs:\n",
    "    ---------- \n",
    "    params: parameter estimate \n",
    "    \"\"\"\n",
    "    d = len(x0) \n",
    "    g = autograd.jacobian(f)\n",
    "    nabla = g(x0)\n",
    "\n",
    "    H = np.eye(d) \n",
    "    params = x0[:]\n",
    "    it = 1\n",
    "    while it < max_it:\n",
    "        it += 1\n",
    "        ###\n",
    "        # A compléter\n",
    "        # search direction p and step size with line search\n",
    "        ###\n",
    "        p = -H@nabla \n",
    "        a = line_search(f,params,p,nabla) \n",
    "        s = a * p \n",
    "        \n",
    "        params_new = params + a * p \n",
    "        nabla_new = g(params_new)\n",
    "        \n",
    "        y = nabla_new - nabla \n",
    "        y = np.array([y])\n",
    "        s = np.array([s])\n",
    "        y = np.reshape(y,(d,1))\n",
    "        s = np.reshape(s,(d,1))\n",
    "\n",
    "        r = 1/(y.T@s)\n",
    " \n",
    "        li = (np.eye(d)-(r*((s@(y.T)))))\n",
    "        ri = (np.eye(d)-(r*((y@(s.T)))))\n",
    "        hess_inter = li@H@ri\n",
    "        H = hess_inter + (r*((s@(s.T)))) \n",
    "        \n",
    "        nabla = nabla_new[:] \n",
    "        params = params_new[:]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1685783529596,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "zolxZZ91Gimu",
    "outputId": "fe88028c-f688-412f-d307-3ffce6f7b522",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19, 0.02])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = BFGS_loop(f,x0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6qWvldWYZd-",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###### **Built-in optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1685781148578,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "ZExfhFFHYZtW",
    "outputId": "5499aab8-5e5c-411f-acc2-dd4ce576f645",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 1.763131196012412e-22\n",
       "        x: [ 1.000e+00  1.000e+00]\n",
       "      nit: 58\n",
       "      jac: [ 3.736e-10 -1.958e-10]\n",
       " hess_inv: [[ 4.965e-01  9.925e-01]\n",
       "            [ 9.925e-01  1.989e+00]]\n",
       "     nfev: 78\n",
       "     njev: 78"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(f, x0, method='BFGS', precision='float64', tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0OpQ8-xQg7g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Application to Design</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbAtb_-aQm0N"
   },
   "source": [
    "###### **Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RHgYIBNQm_H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0o5tPYpQnIv"
   },
   "source": [
    "###### **Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGif2JlSQnTr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
