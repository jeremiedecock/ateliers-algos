{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF7Q8jEeEPM1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=darkblue> Optimization using second order gradient descent </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG6cr03iSYt6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this notebook, we provide an **overview of second order gradient-based optimization algorithms** which completes the first session **on first order methods and accelerated algorithms**. Supervised learning applications are usually based on the minimization of an objective function on $\\mathbb{R}^d$ (kernel based SVM models, penalized regression, maximum likelihood estimation of neural networks) and accelerated gradient methods are the go-to solutions to solve these optimization problems.\n",
    "\n",
    "The results provided in this notebook are valid with assumptions on the target functions such as convexity or strong convexity (some of them may be relaxed). Although these algorithms are widely used, keep in mind that these assumptions do not hold in practice and that  non-convexity stems from the arbitrary form of the loss functions used in machine learning.\n",
    "\n",
    "This will motivate **additional sessions on alternative methods such as evolutionary approaches for difficult non-linear non-convex optimisation problems** in continuous and discrete domains (for instance the CMA-ES algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv1tTxcDjLW6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <font color=darkred> Bibliography & additional ressources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLsK5IHEjPWb",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [1] Convex Optimization, S. Boyd & L. Vandenberghe, 2009, https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n",
    "``Very complete book on convex optimization and gradient descent algorithms (full gradient, Newton's method, constrained problems)``\n",
    ", etc.\n",
    "- [2] Convex Optimization: Algorithms and\n",
    "Complexity, S. Bubeck, 2015, https://arxiv.org/pdf/1405.4980.pdf\n",
    "\n",
    "-  [3] Probabilistic machine learning: an introduction, Kevin P. Murphy, 2022, https://probml.github.io/pml-book/book1.html\n",
    "``Full book online with all basics on machine learning. Not state-of-the-art but very good introduction``\n",
    "\n",
    "- [4] Learning theory from first principles, F. Bach, 2023, https://www.di.ens.fr/~fbach/ltfp_book.pdf\n",
    "``Much more advanced reference, Chapter 5 on optimization``\n",
    "\n",
    "- [5] Algorithms for optimization, M. J. Kochenderfer and T. A. Wheeler, 2019.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B61KWCTMHjsy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred>Introduction : general framework & motivations</font>\n",
    "\n",
    "In machine learning applications, parameter inference often boils down to solving $\\mathrm{argmin}_{\\theta \\in \\mathbb{R}^d} \\,\\{f(\\theta)=\\frac 1n \\sum_{i=1}^n \\ell(\\theta, y_i, x_i) + \\lambda \\mathrm{pen}(\\theta)\\}$, with $\\lambda>0$, $\\mathrm{pen}(\\cdot)$ some penalization function and $(x_i,y_i)_{1\\leq i\\leq n}$ are ``training examples of inputs and outputs`` (in a supervised setting), and $\\theta$ is an ``unknown parameter to be estimated.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this notebook, we do not focus on machine learning applications and **consider a generic target function $f: \\mathbb{R}^d\\to \\mathbb{R}$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8502,
     "status": "ok",
     "timestamp": 1685965004542,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "SQHBC-8OJxKK",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 25,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \"#93a1a1\",\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ0d_yRIRIDw",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Simple cost function to test the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EOokWxB4IRB",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We may consider the Booth and Rozenbrock functions, see fo instance [5], to test the optimization procedures presented in this session:\n",
    "$$\n",
    "f_{\\mathrm{booth}} : (x_1,x_2) \\mapsto (x_1+2x_2-7)^2 + (2x_1+x_2-5)^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "f_{\\mathrm{roz}} : (x_1,x_2) \\mapsto (1-x_1)^2 + 100(x_2-x_1^2)^2\\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1685965133283,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "inBtkl2WGp56",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rozenbrock(x):\n",
    "    return (1-x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def booth(x, y):\n",
    "    return (x + 2*y - 7)**2 + (2*x + y - 5)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "executionInfo": {
     "elapsed": 1831,
     "status": "ok",
     "timestamp": 1685965163242,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "7w6hF96QGqBY",
    "outputId": "cc3cfd2e-24ad-41f9-a21d-3f6eb0007e4e",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the surface\n",
    "x = np.arange(-4, 4, 0.1)\n",
    "y = np.arange(-4, 4, 0.1)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = booth(x, y)\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxDZ_IL_GF4v",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Newton's method </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0oEotU_NHkP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $\\theta_k$ be the current parameter estimate. We consider the second order Taylor expansion of the target $f$. \n",
    "\n",
    "More precisely, this approach proposes to locally **approximate the function $f$ around $\\theta_k$ by a second-order Taylor expansion**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From https://francisbach.com/self-concordant-analysis-newton/\n",
    "![](https://drive.google.com/uc?export=view&id=1aQz3wROslBIqRB9ih_zr3LEES6LWPS0J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the case where $\\nabla^2 f(\\theta_k)$ is definite positive, the function\n",
    "$$\n",
    "\\theta\\mapsto f(\\theta_k) + \\theta^\\top\\nabla f(\\theta_k) + \\frac{1}{2}\\theta^\\top\\nabla^2  f(\\theta_k)\\theta\n",
    "$$\n",
    "admits a unique minimum at \n",
    "$$\n",
    "\\theta_* = -(\\nabla^2 f(\\theta_k))^{-1}\\nabla f(\\theta_k)\\,.\n",
    "$$\n",
    "This motivates the following iterative algorithm:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - (\\nabla^2 f(\\theta_k))^{-1}\\nabla f(\\theta_k)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juGqbmMcP52f",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Implementation from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1685965510806,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "0nvHgIkiDTuc",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685965511526,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "H7RttK4rF1jK",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NR_update(f, x):\n",
    "    # Compute Jacobian and Hession matrices\n",
    "    g = autograd.jacobian(f)\n",
    "    h = autograd.hessian(f)\n",
    "    # Evalute at x\n",
    "    gx = np.squeeze(g(x))\n",
    "    hx = np.squeeze(h(x))\n",
    "    #####\n",
    "    # A compléter\n",
    "    delta = np.linalg.multi_dot([np.linalg.inv(hx), gx])\n",
    "    x = x - delta\n",
    "    #####\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685965511528,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "JDHIpBjkGZ35",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NR_loop(f: callable, x0: np.ndarray, iterations: int = 50) -> tuple[np.ndarray]:\n",
    "    \"\"\"Compute iterative Newton updates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f:\n",
    "        Function to be optimized.\n",
    "    x0: \n",
    "        Initial parameter estimate.\n",
    "    iterations:\n",
    "        Number of updates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    params:\n",
    "        Estimators\n",
    "    loglik:\n",
    "        History of loglikelihood along iterations.\n",
    "    \"\"\"\n",
    "    params = x0\n",
    "    loglik = f(x0)\n",
    "    for _ in range(iterations):\n",
    "        new_params = NR_update(f, params)\n",
    "        loglik = np.append(loglik,f(new_params))\n",
    "        params = new_params\n",
    "    return params,  loglik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "radpjQWr6G6G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Compute the gradient and the Hessian of the Rozenbrock function. \n",
    "\n",
    "- Write an iteration of the Newton update for this function\n",
    "\n",
    "$$\n",
    "f_{\\mathrm{roz}} : (x_1,x_2) \\mapsto (1-x_1)^2 + 100(x_2-x_1^2)^2\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz_BD9jy91dF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradient is given by\n",
    "$$\n",
    "\\nabla f_{\\mathrm{roz}} (x_1,x_2) = \\begin{pmatrix} -2(1-x_1) - 400x_1(x_2-x_1^2) \\\\ 200(x_2-x_1^2)\\end{pmatrix}\n",
    "$$\n",
    "and the Hessian matrix by\n",
    "$$\n",
    "\\nabla^2 f_{\\mathrm{roz}} (x_1,x_2) = \\begin{pmatrix} 2 - 400x_2 + 1200x^2_1 & -400x_1 \\\\ -400 x_1 & 200\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This provides the following iterative algorithm:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\begin{pmatrix} 2 - 400\\theta_k(2) + 1200\\theta^2_k(1) & -400\\theta_k(1) \\\\ -400 \\theta_k(1) & 200\\end{pmatrix}^{-1}\\begin{pmatrix} -2(1-\\theta_k(1)) - 400\\theta_k(1)(\\theta_k(2)-\\theta^2_k(1)) \\\\ 200(\\theta_k(2)-\\theta^2_k(1))\\end{pmatrix}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = rozenbrock\n",
    "iterations = 10\n",
    "for exp_it in range(25):\n",
    "    x0 = 5*np.random.randn(2)\n",
    "    params,  lik = NR_loop(f, x0, iterations)\n",
    "    plt.plot(lik, '-', lw=2)\n",
    "# Configure plot\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Newton Gradient descent - random initialization')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhz2ybCpQcCm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### **Convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uUq9t2INi1D",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assum that the target function $f$ is convex and twice differentiable. Assume also that:\n",
    "- $\\nabla f$ is $L$-Lipschitz and $\\nabla^2 f$ is $M$-Lipschitz ;\n",
    "- $f$ is strongly convex with parameter $m$.\n",
    "\n",
    "Then, there exists a constant $c$, for all $k>0$,\n",
    "$$\n",
    "f(x_{k+k_0}) - f(x_*) \\leq c \\left(1/2\\right)^{2^k}\\,,\n",
    "$$\n",
    "where $k_0$ is the number of steps until $\\|\\nabla f(x_{k_0+1})\\|$ is below a fixed threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Fa_9FBKZls",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Locally, the method is quadratically convergent**, that is, there exists $c>0$, such that if $\\|x_k –x_*\\|\\leq c$, then $\\|x_{k+1} –x_*\\|/c\\leq (\\|x_k –x_*\\|/c)^2$. Thies yields\n",
    "$$\n",
    "\\|x_k –x_*\\|⩽c(\\|x_{k_0} –x_*\\|/c)^{2^{k-k_0}},\n",
    "$$\n",
    "where $k_0$ is such that $\\|x_k –x_*\\|\\leq c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47rtfTg21XPJ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Newton's method is **sensitive to initial conditions**, in particular for non-convex objective functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Giv6s2a134l",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Newton’s method is **very computationally intensive**. The computation of the inverse Hessian scales as **$O(d^3)$** which is prohibitive in high dimensional settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH-_7I7GT7Wd",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Built-in optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1685965738449,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "KOxAH73RT6i_",
    "outputId": "0a692ccf-6907-4c5a-bbea-2ea3d1486820",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from autograd_minimize import minimize\n",
    "x0 = 5*np.random.randn(2)\n",
    "minimize(f, x0, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4818,
     "status": "ok",
     "timestamp": 1685965977238,
     "user": {
      "displayName": "Max Cohen",
      "userId": "07045204525464189350"
     },
     "user_tz": -120
    },
    "id": "hHtZ0_4yWbkz",
    "outputId": "db589caa-443c-4ef3-f6b1-d951268e6322",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try switching `torch` to `tensorflow` as backend\n",
    "minimize(f, x0, method='Newton-CG', precision='float64', tol=1e-8, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kr6kYipGenw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Quasi Newton - Broyden-Fletcher-Goldfarb-Shanno (BFGS) method</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FpBRuVQx5f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computation of the Hessian matrix is sometimes **intractable or computationally very sensitive**. Quasi-Newton approaches propose to use an update similar to the Newton Raphson algorithm with an approximation of the Hessian, i.e.\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\gamma_k A_k^{-1}\\nabla f(\\theta_k)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMSGdNb2oiM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this framework, $A_k$ aims at approximating $\\nabla^2 f(\\theta_k)$. This approximation satisfies in general the quasi-Newton condition:\n",
    "$$\n",
    "A_{k+1}(\\theta_{k+1} - \\theta_k) = \\nabla f(\\theta_{k+1}) - \\nabla f(\\theta_{k})\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND-cEwafRPCK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the BFGS setting, the approximation of the Hessian matrix is computed recursively as follows.\n",
    "- $\\delta_k = \\theta_{k+1}-\\theta_k$.\n",
    "- $d_k = \\nabla f(\\theta_{k+1}) -\\nabla f(\\theta_k)$. \n",
    "- $A_{k+1} = A_k + \\left(d_k^\\top \\delta_k\\right)^{-1}d_kd_k^\\top - \\left(\\delta_k^\\top A_k\\delta_k\\right)^{-1}A_k\\delta_k(A_k\\delta_k)^\\top$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk3htXX-TwL1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using **Woodbury formula**, we can directly compute $A_{k+1}^{-1}$ as follows.\n",
    "\\begin{align*}\n",
    "\\alpha_k &= (d_k^\\top \\delta_k)^{-1}\\,,\\\\\n",
    "A_{k+1}^{-1} &= \\left(I - \\alpha_k \\delta_k d_k^\\top\\right)A_{k}^{-1}\\left(I - \\alpha_k d_k\\delta_k^\\top\\right) + \\alpha_k \\delta_k \\delta_k^\\top\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive BFGS algorithm with constant stepsize\n",
    "# See https://github.com/trsav/bfgs\n",
    "def BFGS_naive(func: callable, x0: np.ndarray, iterations: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "        Function to be optimised. \n",
    "    x0:\n",
    "        Starting point.\n",
    "    iterations: \n",
    "        Number of iterations.\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    Parameter estimate. \n",
    "    \"\"\"\n",
    "    d = len(x0)\n",
    "    g = autograd.jacobian(func)\n",
    "    nabla = g(x0)\n",
    "    H = np.eye(d) \n",
    "    params = x0[:]\n",
    "    for _ in range(iterations):\n",
    "        p = -H@nabla \n",
    "        # no optimization of the step-size\n",
    "        gamma = .1 \n",
    "        s = gamma * p \n",
    "        \n",
    "        params_new = params + s\n",
    "        nabla_new = g(params_new)\n",
    "        \n",
    "        y = nabla_new - nabla \n",
    "        y = np.array([y])\n",
    "        s = np.array([s])\n",
    "        y = np.reshape(y,(d,1))\n",
    "        s = np.reshape(s,(d,1))\n",
    "\n",
    "        r = 1/(y.T@s)\n",
    " \n",
    "        li = (np.eye(d)-(r*((s@(y.T)))))\n",
    "        ri = (np.eye(d)-(r*((y@(s.T)))))\n",
    "        hess_inter = li@H@ri\n",
    "        H = hess_inter + (r*((s@(s.T)))) \n",
    "        \n",
    "        nabla = nabla_new[:] \n",
    "        params = params_new[:]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt = BFGS_naive(f,np.array([.2,1]),100)\n",
    "x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feH5WLEcYRvC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### **Line search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each iteration of a line search method decides the step-size $\\gamma_k$ i.e. the **amplitude of the update**. The iteration is given by\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_{k} + \\gamma_k p_k\\,,\n",
    "$$\n",
    "where $p_k = - A_k^{-1}\\nabla f(\\theta_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most line search algorithms require $p_k$ to be a descent direction:\n",
    "$$\n",
    "p_k^\\top \\nabla f(\\theta_k) <0\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Choosing $A_k$ as the identity matrix yields first order optimization algorithms, while in Newton’s method, $A_k$ is the exact Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ideal choice of the step-size is the global minimizer of $\\alpha \\mapsto f(\\theta_k + \\alpha p_k)$ which is computationally intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From [6]\n",
    "![](https://drive.google.com/uc?export=view&id=1yL5A7FPZ274tLbNOuuITY1JufwPmUI-C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk62oPCoYSCt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing the step length $\\gamma_k$ amounts to solving a tradeoff between decreasing the value of $f$ and not spending too much time at each iteration.\n",
    "\n",
    "A popular line search condition targets to choose $\\gamma_k$ such that:\n",
    "$$\n",
    "f(\\theta_k + \\gamma_kp_k ) \\leq f (\\theta_k ) + c_1 \\gamma_k \\nabla f(\\theta_k)^\\top p_k\\,,\n",
    "$$\n",
    "for $p_k = -A_k^{-1}\\nabla f(\\theta_k)$ some constant $0<c_1<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From [6]\n",
    "![](https://drive.google.com/uc?export=view&id=1rTI4Hxo5EOkBnApX5pEDrj-TxPxpm3v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A second requirement to avoid too short steps requires that\n",
    "$$\n",
    "\\nabla f(\\theta_k + \\gamma_k p_k )^\\top\n",
    "p_k \\geq c_2 \\nabla f(\\theta_k)^\\top p_k\n",
    "$$\n",
    "where $c_1<c_2<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From [6]\n",
    "![](https://drive.google.com/uc?export=view&id=1XJa8Wc34oaV-iyxgAxO4hh2U13_aUqCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S62gptpbRdMQ",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Implementation from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfva2s5Tg1xS",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def line_search(\n",
    "    func: callable,\n",
    "    x: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    nabla: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    f:\n",
    "        Function to be optimized.\n",
    "    x:\n",
    "        Current parameter estimate.\n",
    "    p:\n",
    "        Current value of `p`.\n",
    "    nabla:\n",
    "        Current value of the gradient. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The step-size gamma.\n",
    "    \"\"\"\n",
    "    g = autograd.jacobian(func)\n",
    "    # Initialization\n",
    "    gamma = 1\n",
    "    c1 = 1e-4\n",
    "    c2 = 0.9\n",
    "    \n",
    "    fx = func(x)\n",
    "    x_new = x + gamma * p \n",
    "    nabla_new = g(x_new)\n",
    "    ###\n",
    "    # A compléter\n",
    "    ####\n",
    "    it = 0\n",
    "    while func(x_new) >= fx + (c1*gamma*nabla.T@p) or nabla_new.T@p <= c2*nabla.T@p and it <=50: \n",
    "        it += 1\n",
    "        gamma *= 0.5\n",
    "        x_new = x + gamma * p \n",
    "        nabla_new = g(x_new)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1685783515602,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "qb1xANmrGifT",
    "outputId": "01c36aa1-377f-4112-b912-c0d2d84b1be0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def BFGS_loop(func: callable, x0: np.ndarray, iterations: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "        Function to be optimised. \n",
    "    x0:\n",
    "        Starting point.\n",
    "    iterations: \n",
    "        Number of iterations.\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    Parameter estimate. \n",
    "    \"\"\"\n",
    "    d = len(x0)\n",
    "    g = autograd.jacobian(func)\n",
    "    nabla = g(x0)\n",
    "    H = np.eye(d) \n",
    "    params = x0[:]\n",
    "    for _ in range(iterations):\n",
    "        print(params)\n",
    "        \n",
    "        ###\n",
    "        # A compléter\n",
    "        # search direction p and step size with line search\n",
    "        ###\n",
    "        p = -H@nabla \n",
    "        gamma = line_search(f,params,p,nabla) \n",
    "        s = gamma * p \n",
    "        \n",
    "        print(gamma)\n",
    "        params_new = params + s\n",
    "        nabla_new = g(params_new)\n",
    "        \n",
    "        y = nabla_new - nabla \n",
    "        y = np.array([y])\n",
    "        s = np.array([s])\n",
    "        y = np.reshape(y,(d,1))\n",
    "        s = np.reshape(s,(d,1))\n",
    "\n",
    "        r = 1/(y.T@s)\n",
    " \n",
    "        li = (np.eye(d)-(r*((s@(y.T)))))\n",
    "        ri = (np.eye(d)-(r*((y@(s.T)))))\n",
    "        hess_inter = li@H@ri\n",
    "        H = hess_inter + (r*((s@(s.T)))) \n",
    "        \n",
    "        nabla = nabla_new[:] \n",
    "        params = params_new[:]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1685783529596,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "zolxZZ91Gimu",
    "outputId": "fe88028c-f688-412f-d307-3ffce6f7b522",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "params = BFGS_loop(f,np.array([.2,1]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6qWvldWYZd-",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Built-in optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1685781148578,
     "user": {
      "displayName": "Sylvain Le Corff",
      "userId": "05825960430574299559"
     },
     "user_tz": -120
    },
    "id": "ZExfhFFHYZtW",
    "outputId": "5499aab8-5e5c-411f-acc2-dd4ce576f645",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "minimize(f, x0, method='BFGS', precision='float64', tol=1e-8, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "###### **Comparison with first order methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this section we compare second order approaches with first order optimization methods such as the **full gradient descent** and **Nesterov accelerated algorithm** (see first session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, nabla, w0, iterations, step_size):\n",
    "    loss_history = []\n",
    "    w = w0.copy()\n",
    "    for k in range(iterations):\n",
    "        w = w - step_size * nabla(w)\n",
    "        loss_history.append(f(w))\n",
    "    return w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def nesterov_gd(f, nabla, w, iterations, step_size, beta):\n",
    "    loss_history = []\n",
    "    v = w.copy()\n",
    "    for k in range(iterations):\n",
    "        v_new = w - step_size * nabla(w)\n",
    "        w = v + beta * (v_new - v)\n",
    "        v = v_new\n",
    "        loss_history.append(f(w))\n",
    "    return w, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Compare Newton or Quasi Newton approach with these first order approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f = rozenbrock\n",
    "nabla = autograd.jacobian(f)\n",
    "\n",
    "beta = 0.95\n",
    "step_size = 1e-3\n",
    "iterations = 30\n",
    "w0 = np.random.randn(2)\n",
    "\n",
    "# Compute Nesterov gradient descent\n",
    "w, loss_history = nesterov_gd(f, nabla, w0, iterations, step_size, beta)\n",
    "\n",
    "# Compute full gradient descent\n",
    "w_full, loss_history_full = gradient_descent(f, nabla, w0, iterations, step_size)\n",
    "\n",
    "plt.plot(loss_history_full, '-', lw=3, label=\"Full gradient\")\n",
    "plt.plot(loss_history, lw=3, label=\"Nesterov accelerated\")\n",
    "# Configure plot\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.tight_layout()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0OpQ8-xQg7g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> Application to Design</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbAtb_-aQm0N"
   },
   "source": [
    "###### **Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RHgYIBNQm_H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0o5tPYpQnIv"
   },
   "source": [
    "###### **Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGif2JlSQnTr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred>Appendix: introduction to Jax</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Jax](https://github.com/google/jax) allows to use python and numpy functions with automatic gradient computation, and fast linear algebra through just in time (jit) compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Basic algebra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "W = jnp.zeros((4,4))\n",
    "x = jnp.ones((2))\n",
    "jnp.dot(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`grad` outputs the gradient of any function $f$ given in input. \n",
    ". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "grad_tanh = grad(jnp.tanh)\n",
    "print(grad_tanh(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Jacobian and Hessain matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Jax allows to compute Jacobian matrices using the `jacfwd` and `jacrev` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
